{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4pRqO3AMTMNo"
   },
   "source": [
    "# Text classification with Transformer\n",
    "\n",
    "**Author:** [Divyanshu Raghuwanshi](https://www.linkedin.com/in/divyanshu-raghuwanshi-85037b160/)<br>\n",
    "**Date created:** 2021/03/22<br>\n",
    "**Last modified:** 2021/31/03<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xnAENkcHTMNt"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "7Aie5cs0TMNu"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from numpy import dstack\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from google.colab import drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ElFCg3Gpcn3N",
    "outputId": "78ee0a23-e335-46df-fd3e-f041b345c2d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xlsxwriter\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/34/eb/40aaf7a73fd158aea04ad8812b97fd3049929276c9ea652d8a995cd18425/XlsxWriter-1.3.8-py2.py3-none-any.whl (145kB)\n",
      "\r",
      "\u001b[K     |██▎                             | 10kB 19.3MB/s eta 0:00:01\r",
      "\u001b[K     |████▌                           | 20kB 11.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████▊                         | 30kB 14.0MB/s eta 0:00:01\r",
      "\u001b[K     |█████████                       | 40kB 16.0MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▎                    | 51kB 16.6MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▌                  | 61kB 14.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▉                | 71kB 14.0MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████              | 81kB 12.1MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▎           | 92kB 12.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▌         | 102kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▉       | 112kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████     | 122kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▎  | 133kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▋| 143kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 153kB 12.2MB/s \n",
      "\u001b[?25hInstalling collected packages: xlsxwriter\n",
      "Successfully installed xlsxwriter-1.3.8\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install xlsxwriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "XXyRLs0Mcssk"
   },
   "outputs": [],
   "source": [
    "import xlsxwriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HbI2ad1PTMNu"
   },
   "source": [
    "## Implement a Transformer block as a layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "Kq4bDSvrTMNv"
   },
   "outputs": [],
   "source": [
    "\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BepQG-vwTMNv"
   },
   "source": [
    "## Implement embedding layer\n",
    "\n",
    "Two seperate embedding layers, one for tokens, one for token index (positions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "HDNFDgb_Yb9y"
   },
   "outputs": [],
   "source": [
    "class PositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, embed_dim):\n",
    "        super(PositionEmbedding, self).__init__()\n",
    "        #self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-2]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        #x = self.token_emb(x)\n",
    "        return x + positions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j53nit2lTMNw"
   },
   "source": [
    "## Download and prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AoML4kDRqiFv",
    "outputId": "6fe0e54f-105d-45be-b155-93f1d7d3a1db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "PiZBLiK-qnjU"
   },
   "outputs": [],
   "source": [
    "df= pd.read_csv('/content/drive/MyDrive/Research data/MeggitSummaryCSV.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "dr70GkjCreYr"
   },
   "outputs": [],
   "source": [
    "Xdf=df.iloc[:,2:18]\n",
    "Xdf=Xdf.values\n",
    "ydf=df.iloc[:,-1]\n",
    "ydf=ydf.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "pdHxi5wcreVa"
   },
   "outputs": [],
   "source": [
    "def prepareData(X,y,W,s_W):\n",
    "  #X,y = removeNAN(X,y)\n",
    "  #X,y = removeNULL(X,y)\n",
    "  # Standardizing with z-score\n",
    "  #X = stats.zscore(X,axis = 0)\n",
    "  X_data=list()\n",
    "  y_data=list()\n",
    "  \n",
    "  L=0\n",
    "  R=W\n",
    "  \n",
    "  while(R <= X.shape[0]):\n",
    "      #print('{}:{}'.format(L,R))\n",
    "      sample=X[L:R]\n",
    "      label=y[L:R]\n",
    "      if len(set(label))==1 and len(label)==W:\n",
    "          X_data.append(sample)\n",
    "          y_data.append(label[0])\n",
    "\n",
    "      L=L+s_W\n",
    "      R=R+s_W\n",
    "\n",
    "  X=np.array(X_data)\n",
    "  y=np.array(y_data)\n",
    "  return X,y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "CSuCIjCYreST"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "W=120\n",
    "s_W=60\n",
    "#dataX, datay = prepareData(X,y,W,s_W)\n",
    "dataX, datay = prepareData(Xdf,ydf,W,s_W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1d7nywtPr4D3",
    "outputId": "f4aee533-e3eb-4150-c7e2-f7da1c76e75d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6567, 120, 16), (6567,))"
      ]
     },
     "execution_count": 53,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataX.shape,datay.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "vjaTV6_Z2deW"
   },
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "  X_train, X_val, y_train, y_val = train_test_split(dataX[:,:,:], datay[:], test_size=0.20, random_state=1)\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X_train[:,:,:], y_train[:], test_size=0.10, random_state=1)\n",
    "  return X_train, X_test, X_val, y_train, y_test, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "OKq24M3tT34r"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "9uJ2LrFOVPgm"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LR_dijOJTMNw"
   },
   "source": [
    "## Create classifier model using transformer layer\n",
    "\n",
    "Transformer layer outputs one vector for each time step of our input sequence.\n",
    "Here, we take the mean across all time steps and\n",
    "use a feed forward network on top of it to classify text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hWy2rPIMTMNx"
   },
   "source": [
    "## Train and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "lg1ASGyKvUrV"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(X_train, y_train, X_val, y_val, X_test, y_test):\n",
    "    maxlen=120\n",
    "    embed_dim = 16  # Embedding size for each token\n",
    "    num_heads = 2  # Number of attention heads\n",
    "    ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
    "    epochs = 1\n",
    "    Verbose = 0\n",
    "    Batch_size=84\n",
    "\n",
    "    inputs = layers.Input(shape=(maxlen,embed_dim))\n",
    "    embedding_layer = PositionEmbedding(maxlen, embed_dim)\n",
    "    x = embedding_layer(inputs)\n",
    "    transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "    x = transformer_block(x)\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    x = layers.Dense(20, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    outputs = layers.Dense(6, activation=\"softmax\")(x)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    model.compile(\"adam\", \"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    model.fit(X_train, y_train, batch_size=Batch_size, epochs=epochs, validation_data=(X_val, y_val), verbose=Verbose)\n",
    "\n",
    "    # evaluate model\n",
    "    _, accuracy = model.evaluate(X_test, y_test, batch_size=Batch_size, verbose=Verbose)\n",
    "  \t\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "Fui5I-B-r8bc"
   },
   "outputs": [],
   "source": [
    "def summarize_results(scores):\n",
    "    print(scores)\n",
    "    m, s = mean(scores), std(scores)\n",
    "    print('Accuracy: %.3f%% (+/-%.3f)' % (m, s))\n",
    "    \n",
    "    fullCatWriter = pd.ExcelWriter('/Result.xlsx', engine='xlsxwriter')     \n",
    "    dataframe1 = pd.DataFrame(scores)            \n",
    "    dataframe1.to_excel(fullCatWriter, sheet_name='Full Raw', startrow=0 , startcol=0)            \n",
    "    fullCatWriter.save()      \n",
    "    fullCatWriter.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "RVnpY-Ydu4vR"
   },
   "outputs": [],
   "source": [
    "def run_experiment(repeats=5):\n",
    "\t# load data\n",
    "\t# repeat experiment\n",
    "  scores = list()\n",
    "  X_train, X_test, X_val, y_train, y_test, y_val = load_dataset()\n",
    "  for r in range(repeats):\n",
    "    score = evaluate_model(X_train, y_train, X_val, y_val, X_test, y_test)\n",
    "    score = score * 100.0\n",
    "    print('>#%d: %.3f' % (r+1, score))\n",
    "    scores.append(score)\n",
    "  \n",
    "\t# summarize results\n",
    "  summarize_results(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6YxsPpxgu7bt",
    "outputId": "2d94bb39-8653-475c-aa4d-0eb62a62d084"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">#1: 51.331\n",
      ">#2: 71.863\n",
      ">#3: 67.871\n",
      ">#4: 57.795\n",
      ">#5: 58.365\n",
      "[51.330798864364624, 71.86312079429626, 67.87072420120239, 57.79467821121216, 58.36501717567444]\n",
      "Accuracy: 61.445% (+/-7.415)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/xlsxwriter/workbook.py:336: UserWarning: Calling close() on already closed file.\n",
      "  warn(\"Calling close() on already closed file.\")\n"
     ]
    }
   ],
   "source": [
    "run_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jlTVCeZbXlSM"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "(FINAL)TRANSFORMER-including_positional_encoding",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
